# -*- coding: utf-8 -*-
"""2nd_architecture

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Co5Di1OUs492Pl0tZcl1Zl5oSjDYrKyD
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization
import os
import pandas as pd
import numpy as np

base_dir=r'/content/drive/MyDrive/Feature optimization/Covid lungs sample'

img_h,img_w= (224,224)
batch_size=64

model = Sequential()

model.add(Conv2D(64, (3, 3), strides=(1,1),input_shape=(img_h,img_w,3)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))


model.add(Conv2D(64, (3, 3), strides=(1,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))


model.add(Conv2D(32, (3, 3), strides=(1,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))

model.add(Conv2D(16, (3, 3), strides=(1,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))

model.add(Conv2D(8, (3, 3), strides=(1,1)))


model.add(Flatten())
model.add(Dense(3, activation='relu'))

model.add(Dense(1, activation='sigmoid'))

model.summary()

from tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop
from tensorflow.keras.callbacks import ReduceLROnPlateau


reduce_learning_rate = ReduceLROnPlateau(monitor='loss',
                                         factor=0.1,
                                         patience=3,
                                         cooldown=2,
                                         min_lr=1e-10,
                                         verbose=1)

#from tensorflow.keras.callbacks import ModelCheckpoint
checkpoint =tf.keras.callbacks.ModelCheckpoint(filepath="Covid.h5", 
                            monitor='val_accuracy',
                            verbose=1,
                            save_best_only=True, 
                            save_weights_only=False, 
                            mode='auto',
                            save_freq='epoch')




#early = EarlyStopping(monitor='val_accuracy', patience=30, verbose=1, mode='auto')
callbacks = [reduce_learning_rate, checkpoint]
optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile( loss='binary_crossentropy',optimizer= optimizer, metrics=['accuracy'])

from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen= ImageDataGenerator(rescale=1./255,
                            validation_split=0.2)

train_generator = datagen.flow_from_directory(
        base_dir,  # This is the source directory for training images
        target_size=(img_h, img_w),  
        batch_size=batch_size,
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode='binary',
        #color_mode = 'grayscale',
        subset='training')

validation_generator = datagen.flow_from_directory(
        base_dir,  # This is the source directory for training images
        target_size=(img_h, img_w),  
        batch_size=batch_size,
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode='binary',
        #color_mode = 'grayscale',
        subset='validation')

model.fit(
    train_generator,
    steps_per_epoch = train_generator.samples // batch_size,
    validation_data = validation_generator, 
    validation_steps = validation_generator.samples // batch_size,
    epochs = 20,
    verbose=1,
    callbacks= callbacks)

from tensorflow.keras.models import Model
feature_extractor= Model(inputs=model.inputs, outputs=model.layers[16].output)
feature_extractor.summary()

from PIL import Image
import numpy as np
import sys
import os
import csv
from tensorflow.keras.preprocessing.image import load_img,img_to_array
from numpy import expand_dims

#Useful function
def createFileList(myDir, format='.bmp',fr2='.png'):
  fileList = []
  print(myDir)
  for root, dirs, files in os.walk(myDir, topdown=False):
    for name in files:
      if name.endswith(format) or name.endswith(fr2):
        fullName = os.path.join(root, name)
        fileList.append(fullName)
  return fileList

# load the original image
fileList = createFileList('/content/drive/My Drive/Feature optimization/CT_COVID SAMPLES/CT_NonCOVID','.png','.jpg')
label= 1

'''
0 for Covid 
1 for Non_Covid


columns=['label']
for i in range(10*10*8):
    columns.append("F_"+str(i))

with open("CTcovid_feature.csv", 'a') as f:
        writer = csv.writer(f)

        writer.writerow(columns)
'''

for file in fileList:

    print(file)

    

    print(label)

    

    #img = load_img(file,color_mode = "grayscale",target_size=(img_h, img_w))
    img = load_img(file,target_size=(img_h, img_w))
    img = img_to_array(img)
    img= img/255.0

    # expand dimensions so that it represents a single 'sample'
    img = expand_dims(img, axis=0)
    pvalue = feature_extractor.predict(img)


    
    pvalue = pvalue.flatten()
    print(pvalue.shape)
    value=[label]
    for i in pvalue:
        value.append(i)

    with open("CTcovid_feature.csv", 'a') as f:
        writer = csv.writer(f)

        writer.writerow(value)

dataframe= pd.read_csv("/content/CTcovid_feature.csv")
dataframe.head()